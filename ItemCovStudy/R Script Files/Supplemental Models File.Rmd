---
title: "Modeling and Results Supplement"
output: 
  rmdformats::robobook:
    self_contained: true
    thumbnails: false
    lightbox: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose of the Document

The aim of these supplementary details are to provide readers with better documentation of the complete modeling and model selection process. There were a total of 11 models fit as part of this study, and the number of analyses and supporting figures to understand this process are simply too long for a single document. This document therefore provides relevant plots and summaries to help readers understand the steps taken to arrive at the final model, and perhaps more importantly, the document provides commentary for how each modeling choice was made.

The goal of this document is therefore to ensure complete transparency in the data analysis process. Accompanying this document is also the R script file that was created during the study process. Someone who acquires access to the appropriate files through the HRS should be able to run the script file (with some minimal alterations to ensure that the files are read from the correct files) and then get results that match those shown here (within some margin of error due to randomness in some of the methods, though the use of the same seed in R and `brms` should reduce the impact of this).

## Overview of Sections

In an attempt to facilitate readability of this document, the modeling process is broken down into a few different sections. Additionally, there is a table of contents that readers can use to jump to any section, so for those interested in specific aspects of this supplementary material, it is useful to know what exists in each section.

First, a flowchart showing the order in which models were built and the model to which each was tested is provided. Each candidate model was compared to the up-to-then best fitting model. The aim of this flowchart is to help familiarize readers with the models and modeling steps quickly and efficiently as the remaining sections add greater detail.

Second, the priors used in the models are explored. As the data analyses were done in the Bayesian framework, the analysis of the priors is useful. The priors are shown as well as the prior predictive checks for a fixed item model.

Third, the details of each model are provided. This is the largest section as each model includes a variety of explorations. To help reduce the overall length of the document, each model is given its own tab so that readers can select one model at a time. Details for each model can be broken down into 3 primary sections: model validity, model performance, and model estimates. 

*   Model validity refers to tests of whether the estimation process converged and not subject to any issues that would make estimates from the model entirely invalid or unstable. These tests include visual inspection of chain mixtures, $\hat{R}$, effective sample size, and maximum treedepth. 
*   Model performance refers to the posterior predictive checks that mirror those shown in the manuscript for the final model: predictions of all responses for all items, responses to each item, and responses to all items for a subset of participants. As the space of this document is less limited than space in the final manuscript, the random subset of participants is increased from 12 to 20.
*   Model estimates refers to the summary of model parameters like fixed and random effects estimates. This summary is different than the one presented in the manuscript for the final paper as the objective of these intermediary models is not to summarize effect sizes or the probability of these effects; instead, the goal is to get a general idea of what the model is estimating and how it is performing. Toward this end, conditional effects plots for each model are also included. Note that these plots may not be very informative for the majority of the models tested because only a subset included many covariates.

Finally, as discussed in the manuscript, some additional details regarding the final model are also included here. These details include summaries of the item parameters in traditional IRT metrics (i.e., as difficulty and discrimination), the reliability plot, expected score functions (total test), and information functions (total test and all trials). As in the manuscript, these item-specific plots are provided with and without assuming previous learning of the words. The test is best understood as a dynamic one in which a person's performance on each trial changes our expectation for how they will perform on the next.

## What's Being Done

As mentioned earlier, this document shows all the code used to generate the results. Since there is an accompanying R data script, it may be useful for readers to know the objects being called in this Markdown document since those objects can be connected back to the R script. The hope is that this will create a reasonable sense of cohesion between the supplementary materials, and it should mean that all the results here are also fully reproducible. Toward that end, the objects and packages used in this document are shown below (note that the R objects are read in as RDS files whose names are consistent with those listed in the R script file).

```{r SetupData}
#read in needed data
df_long <- readRDS("Data_long.rds")
Rasch_prior <- readRDS("Fitted Models/1PL_prior_check.rds")
TwoPL_prior <- readRDS("Fitted Models/2PL_prior_check.rds")
Rasch_inter <- readRDS("Fitted Models/1PL_intercept.rds")
TwoPL_inter <- readRDS("Fitted Models/2PL_intercept.rds")
TwoPL_learn <- readRDS("Fitted Models/2PL_growthModel.rds")
TwoPL_multi <- readRDS("Fitted Models/2PL_multidimensional.rds")
TwoPL_chnge <- readRDS("Fitted Models/2PL_changeModel.rds")
TwoPL_depmd <- readRDS("Fitted Models/2PL_dependencyModel.rds")
TwoPL_depun <- readRDS("Fitted Models/2PL_dependencyUniqueModel.rds")
TwoPL_deptr <- readRDS("Fitted Models/2PL_dependencyTrialModel.rds")
TwoPL_itmex <- readRDS("Fitted Models/2PL_itemCovariates.rds")

#load in required packages 
#wrapping in suppressPackageStartupMessages() done just to reduce print out in document
suppressPackageStartupMessages(library(brms))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(kableExtra))
```

# Modeling Flowchart

As can be inferred from the figure below, the modeling process involved several iterations of fitting similar models and considering results from old models. As there specific hypotheses regarding the model and its covariates, the models needed to test these hypotheses were naturally created as part of the study. At the same time, alternative models had to be specified against which to test these models. Additionally, while there were hypotheses regarding what covariates (and their signs) would ultimately be in the final model, the important result of the study is the final model itself. Some readers may see the number of models examined and variations in their specification and become concerned of potentially two things: multiple comparisons + inflated error rates and/or model fishing/*p*-hacking.

The objective of this supporting document is to help clarify all the modeling choices so that readers do not need to question whether model specifications were made to try to improve performance of the final result. With respect to possible concerns regarding multiple comparisons, Bayesian methods do no suffer from these concerns (Gelman et al., 2013; [Gelman, Hill,& Yajima, 2012](http://www.stat.columbia.edu/~gelman/research/published/multiple2f.pdf); [Neath, Flores, & Cavanaugh, 2017](https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1420); [Sj&ouml;lander & Vansteelandt, 2019](https://link.springer.com/article/10.1007/s10654-019-00517-2)). While there are several reasons that this is the case for Bayesian methods, it is sufficient to speak to three. First, we do not use null hypothesis testing in this study. Model comparisons are completed using a formal comparison of information criteria to select models with better out-of-sample performance. Coefficients are not interpreted as significant or not but instead are summarized in terms of their probability of existing. Since the correction to *p*-values for multiple comparisons is to control the risk of falsely rejecting the null hypothesis, this is not a concern when we are not rejecting null hypotheses. Second, we utilize skeptical priors for our effects estimates. This means that we are *a priori* placing greater probability of the effects being 0 (or practically equivalent to 0). This is the inverse of frequentist decision-making practices where the null hypothesis is very easy to reject since it is constrained to (usually) a nill point value, which is rarely a realistic threshold for any model specification. Finally, the models include skeptical priors, meaning that all effects estimates are pulled closer to those *a priori* small effects.

```{r Flowchart, echo = FALSE, fig.cap = "Flowchart of all models fit and compared in order (click to make larger)"}
knitr::include_graphics("C:\\Users/billy/Desktop/Psych Articles/HCAP/CERAD IRT Update/Figures/ItemCovariateModels_Flowchart.bmp")
```

# Prior Specifications and Inspection

As discussed in the manuscript for this study, prior specification came from documentation on using `brms` for IRT (i.e., [B&uuml;rkner, 2020a](https://arxiv.org/abs/1905.09501) and [B&uuml;rkner, 2020b](https://www.mdpi.com/2079-3200/8/1/5/htm)). As a general note, the non-linear specification for the 2PL model used in this study come from the B$uuml;rkner (2020b) study published in the *Journal of Intelligence*. Also as discussed in the manuscript, the specification of priors follows the recommendations of other typical multilevel regression guides (e.g., Gelman & Hill, 2007). Specifically, the priors are normal distributions with wide variances relative to the scale of the outcome data. As these priors on the regression coefficients, normal distributions are appropriate prior distributions. While these distributions are centered on 0, they are made wide and thus only weakly informative to the final parameter estimates. This specification helps regularize estimation (i.e., pull estimates toward zero and away from more extreme values) while imparting little *a priori* influence on the estimates. Additionally, by making the priors skeptical (i.e., they place the greatest probability on very small to non-existent effects), there is a reduction in the risk of experimenter bias; however, with 1219 participants each observed 30 times, the data will dominate the prior anyway.

The priors for the Rasch and 2PL models are shown below:
```{r PriorSpecifications, eval = FALSE}
Rasch_priors <-
  prior("normal(0, 5)", class = "b") +
  prior("normal(0, 3)", class = "sd", group = "ID")

TwoPL_priors <- 
  prior("normal(0, 5)", class = "b", nlpar = "beta") +
  prior("normal(0, 1)", class = "b", nlpar = "logalpha") +
  prior("normal(0, 1)", class = "sd", group = "ID", nlpar = "theta")
```

Readers following the R script file will recognize that the above are repeated in that document ("3 - Measurement and Explanatory Model Fit", lines 19-27). To read these priors, it can be helpful to look at a couple of examples. Starting with the priors for the Rasch model, the prior for the coefficients (`class = "b"`) is specified as a normal distribution with a mean of zero and standard deviation of five. This would mean that, before looking at the data, we are guessing that there is a 68% probability of the intercept being between -5 and 5 (i.e., +/- 1SD), and we are guessing that there is about a 95% probability that the coefficients will be between -10 and 10 (i.e., +/- 2SD). In these models, the coefficient corresponds to the actual item parameters (for the Rasch, this is item easiness), so these are the ranges in which we are saying are potential values we might observe. Take another example but this time from the 2PL model and a random effect. The random effect priors are all labeled as `class = "sd"` since we are putting a prior belief on the plausible values of the standard deviation of the random effects. For the random person effect (i.e., latent trait of each participant), we look for the variable that defines this group (`group = "ID"`, where ID is a number indexing each participant) and the non-linear element it is estimating (`nlpar = "theta"` where $\theta$ is the traditional IRT simple for the latent trait). The prior therefore for the latent trait is the standard normal distribution with mean of zero and standard deviation of 1. This specification is consistent with the treatment of the latent trait in IRT as normally distributed and z-scaled, though generally in IRT models the variance is constrained to be 1 for identifiability purposes (see B&uuml;rnker, 2020a for details).

While it's comforting to be able to go through each prior specification and think about what it means, it is perhaps more efficient to examine some plots. The first set of plots shown are the parameter estimates returned when the model samples only from the prior. In other words, these are the estimated effects implied by the priors. If the prior specifications are truly skeptical and weakly informative, then they will give the greatest probability to effects of very small size and cover a wide range of plausible values. The prior distribution for the Rasch easiness parameter of "Butter" on trial one. 

```{r RaschPriorEstimates}
plot(Rasch_prior, combo = c("dens", "intervals"), variable = "b_ItemButter1", ask = FALSE)
```

Consistent with expectations, these plots show wide ranges of plausible values with the greatest probabilities being placed on small effects. In the left column are the density plots of the estimates while the right column shows the interval estimates with the circle = mean estimate, bold line = 50% credible interval, and thin line = 90% credible interval. The prior density very clearly follows a normal distribution that allows admissible values with more extreme estimates being given small overall probability of being true. These wide priors allow the data to dominate the posterior estimates for the parameters, though again this would likely be the case with even more informative priors due to the size of the sample available.

```{r 2PLPriorEstimates}
plot(TwoPL_prior, combo = c("dens", "intervals"), variable = "b_beta_ItemButter1", ask = FALSE)
```

The plot above mirrors what was shown for the Rasch model but are for the 2PL model, which is clearly not different as the same priors are used for this parameter. Though all the prior distributions for the model can be shown, this results in many different plots that can be overwhelming to interpret, and these plots correspond to just priors for those specific parameters. Since researchers may not often have explicit expectations for what each parameter will be or should be, these plots are not always directly helpful.

Another related graphical output is the prior predictive check. The prior predictive check runs the model using the priors rather than the observed data (note that this model would be similar to a null model). If the priors are specified well, then they should return reasonable, albeit wide, estimates of the observed data. This plot is more intuitive for understanding the effect of priors instead of parameter-by-parameter plots like above. The prior predictive checks for the Rasch and 2PL models are shown below following the same layout as the posterior predictive checks in the manuscript and for the other models.

```{r RaschPriorPredictiveCheck}
pp_check(Rasch_prior, ndraws = 25, type = "bars")
pp_check(Rasch_prior, ndraws = 25, type = "bars_grouped", group = "Item")
pp_check(Rasch_prior, ndraws = 25, type = "bars_grouped", group = "ID", 
         newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

The Rasch prior predictive checks above demonstrate that the prior specifications are adequately wide to provide coverage for the observed data. The estimates themselves are expectedly poor, which is a product of the skeptical priors. The same plots are now repeated but for the 2PL model.

```{r 2PLPriorPredictiveChecks}
pp_check(TwoPL_prior, ndraws = 25, type = "bars")
pp_check(TwoPL_prior, ndraws = 25, type = "bars_grouped", group = "Item")
pp_check(TwoPL_prior, ndraws = 25, type = "bars_grouped", group = "ID", 
         newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Performance of the 2PL priors are similar to those of the Rasch, suggesting that these priors are also appropriately specified.

While all the above plots and theoretical justifications suggest that the priors are specified consistent with the wishes for the model, it can also be helpful to perform a post-hoc test of whether a model's priors were influential on its final estimates. As has been mentioned multiple times in this section, due to the sample size of this study, it is expected that the data and not the prior will dominate the posterior estimates, meaning that even with more informative priors the data would have more influence on the final estimates. One such comparison discussed by Andrew Gelman (link [here](https://statmodeling.stat.columbia.edu/2019/08/10/for-each-parameter-or-other-qoi-compare-the-posterior-sd-to-the-prior-sd-if-the-posterior-sd-for-any-parameter-or-qoi-is-more-than-0-1-times-the-prior-sd-then-print-out-a-note-the-prior-dist/)) is to compare the posterior standard deviation (i.e., precision of the effect estimate after looking at the data) to the prior standard deviation (i.e., uncertainty of the effect estimate before looking at the data). In the case that a prior is influential, the ratio of the precision to uncertainty will be large. Put another way, we learn little more about the posterior from observing the data because the prior was already highly informative. Gelman's recommended threshold for determining whether a prior is informative is if the posterior standard deviation for an effect is more than 0.1 times the prior standard deviation. The able below provides this metric for each predictor from the final model reported in the study.

```{r PriorSensitivityCheck}
#get the posterior samples from the final model
posteriors <- as_draws_df(TwoPL_itmex)

#get the fixed effects for the item easiness
beta <- posteriors %>%
  select(starts_with("b_beta")) %>%
  apply(., 2, function(x) sd(x)/sd(posteriors$prior_b_beta)) %>%
  as.matrix()

#do the same for item discrimination
alpha <- posteriors %>%
  select(starts_with("b_logalpha")) %>%
  apply(., 2, function(x) sd(x)/sd(posteriors$prior_b_logalpha)) %>%
  as.matrix()

#combine into a single result
result <- rbind(beta, alpha) %>%
  as.data.frame() %>%
  add_column("Prior Influence" = ifelse(.[, 1] >= 0.1, "Informative", "Uninformative")) %>%
  rename("Ratio (Posterior:Prior)" = V1)
row.names(result) <- paste(rep(c("Easiness:", "Discrimination:"), each = 11), rep(c("Intercept", "Trial 2", "Trial 3", "Item Pos. (linear)", "Item Pos. (quadratic)", "Word Frequency", "Concreteness", "Semantic Diversity", "Age of Acquisition", "Body-Object Integration", "Phonemes"), 2))
rm(posteriors, beta, alpha)

#get resulting table
result %>%
  kable(caption = "Comparison of the Posterior to Prior Distribution Standard Deviations", digits = 4, align = 'cc') %>%
  column_spec(1:3, bold = ifelse(result$`Ratio (Posterior:Prior)` >= 0.10, TRUE, FALSE)) %>%
  kable_classic(full_width = FALSE, position = "float_right")
```

The table of these posterior and prior comparison results is shown here. For convenience, only those whose threshold exceeds the recommended > 0.10 ratio are bolded. Generally, the findings suggest that the priors performed as expected: they were weakly informative and did not seemingly have undue influence on the posterior estimates. Notably, the exceptions are the discrimination intercept, which had a 95% credible interval that included zero, and the item position coefficients that had relatively larger standard errors. This is not unexpected as the result essentially indicates that even after looking at the data we do not change our prior beliefs, which were that the effects were either zero or very wide. The influence of the prior on these null findings reflects a point made earlier regarding how Bayesian methods are generally unaffected by multiple comparisons. 

An important point to emphasize at this juncture is the implication of a "significant" finding in Bayesian methods. As discussed throughout this section on priors, the priors here are skeptical of an effect in the sense that they place greatest weight on an effect estimate of zero or close to zero and ambivalence regarding the direction of the effect (i.e., it is just as equally probable that the sign is positive of negative). In the context of the current study, this means that, despite the hypotheses regarding the presence and direction of specific effects, the priors for these predictors are specified in this skeptical way so as to avoid the introduction of experimenter bias. In regard to the robustness of the effects observed, the fact that they are observed from the information provided by the data despite these skeptical priors also helps build confidence in the presence of these effects.

# Model Details {.tabset .tabset-fade}

The details of this section highlight the models fitting and results. These details speak to the validity of the model results and then also the actual results (i.e., parameter estimates) of the model. Model validity is particularly important in Bayesian methods because the parameter estimates are based on Monte Carlo Markov Chains (or Hamiltonian Monte Carlo (HMC) in the case of these models run using *Stan*). In cases where a model fails to converge or throw errors under the estimator, the validity of the model results are questionable or even completely invalid (e.g., in the case of divergent transitions). To reflect this need to first confirm the validity of the results, various diagnostics of the model fit are provided first before then presenting the model results.

For readers unfamiliar with these model checks, a brief overview of each is provided here. The largest threat to model results in the HMC is arguably the presence of divergent transitions. HMC explores the posterior distribution by simulating the evolution of a Hamiltonian system, and in order to do this efficiently, the sampler finds a reasonable step size with which to explore that space. A divergent transition occurs when the trajectory of the system is lost due to too large of a step size. Another important model check is the treedepth of the chains. Again, to improve the efficiency of the posterior sampling, a maximum treedepth is set to prevent the estimator spending excessive time in certain steps and spaces. Since this treedepth may artificially limit the estimator in exploring the posterior space, it is important to check whether any of these treedepths were actually hit during estimation (default treedepth is 10). Another important Bayesian model indicator is $\hat{R}$ because multiple HMC (and MCMC) chains are needed to ensure that the posterior is sampled appropriately. If a single chain is run, then it is not possible to determine whether the random starting values of this chain may have lead to a specific set of parameter estimates. Running multiple independent chains that each have different random starting values helps ensure that the parameter estimates are not biased by exploration of only certain posterior values. In well-behaved models, these chains will mix together without any clear indications of one chain producing a specific set of parameter estimates that differ from what other chains are estimating. While this mixture of chains can be visually inspected via the trace plot (also provided here), the $\hat{R}$ statistic is a simple indicator of this with the conservative recommendation of treating estimates as valid only if the $\hat{R}$ for the parameter is less than 1.01. A final model validity check shown here is the effective sample size. Because multiple chains are run for many samples of the posterior, it is expected that some of those samples are autocorrelated and thus dependent on previous samples. Effective sample size informs us of the precision of the model in MCMC and HMC methods. When samples are independent, the central limit theorem indicates that the precision with which a parameter can be estimated is proportional to the size of the sample (e.g., $\sigma_\bar{x} = \frac{\sigma}{\sqrt{N}}$). The same proportionality can be obtained when samples are dependent but requires replacing $N$ with $N_{ESS}$, or the effective sample size. Due to the dependence of the sampling, $N_{ESS} < N$ and thus precision of the estimate is less than would be if it could be estimated from the total sample. Running the chains for more iterations will necessarily increase $N_{ESS}$, but there is a practical demand on computational effort relative to marginal increases in precision. The recommendation of the *Stan* developers is to run enough iterations of the sampler to obtain an $N_{ESS} >= 4*N_{chains}$. All models were run using 4 independent chains, so the minimally acceptable ESS is 400 (i.e, 4*100).

In the case that the model checks are appropriate, it is appropriate to examine the posterior distribution and begin inference based on the results. While there are more parameters estimated by the models, posterior summaries of the coefficients in each model are shown. The posterior is summarized as a density plot that reflects the probability distribution of the parameter based on integrating our prior knowledge and observed data. The density plot shows the 95% credible interval with the 80% credible interval shown as a shaded area. Unlike frequentist confidence intervals, these credible intervals can be interpreted as the probability of the parameter having a specific value. For example, if the 95% credible ranges from 0.50 to 1.00, then this means that there is a probability of 0.95 that the parameter has a value somewhere within this interval. This is in contrast to frequentist confidence intervals where the same interval would be interpreted as meaning that 95% of point estimates based on the same statistical test applied to an infinite number of random samples of the same population will be within this interval. Thus, where the credible interval directly summarizes the our beliefs about the parameter and our uncertainty about its true value, the confidence interval only reflects point estimates that we would expect to observe if the study and statistical methods were repeated an infinite number of times. Posterior predictive checks of the models are then also presented as they were for the final model in the corresponding manuscript. There is one additional model exploration plot provided in this section that has not been addressed before in this document: the conditional effects plot. As the predictors in these models are correlated and have their effects estimated on the logit scale, it can be challenging to look at the model estimates and understand the implication of these values in an intuitive manner. One way to address this is to visualize how the predicted outcome of the model changes as a function of each predictor while holding all other predictors constant (e.g., at their mean value). The resulting plot is the conditional effects plot. In the case of these models, this plot shows, for each predictor, what happens to the predicted probability of a correct response as the value of the predictor changes and all other model values are held constant. These plots are not realistic as it is not reasonable to assume that there exist words whose traits can vary only on one property at a time; however, they do provide a quick method of understanding the relative effect of each predictor by showing its linear trend as implied by the model. As a result, these plots should not be used for prediction or extrapolation in any regard; instead, if the goal is prediction of responses, then the entire model should be used, and extrapolation of these predictions to values not observed in this study should be avoided. These plots are simply to help contextualize the meaning of the effect estimate in the model.

## Rasch Fixed Items

```{r RaschPlots}
mcmc_plot(Rasch_inter, type = "nuts_divergence")
mcmc_plot(Rasch_inter, type = "nuts_treedepth")
mcmc_plot(Rasch_inter, type = "trace", variable = "b_Item", regex = TRUE)
mcmc_plot(Rasch_inter, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(Rasch_inter, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(Rasch_inter, type = "neff_hist", binwidth = 0.1)
```

The fixed items Rasch model demonstrated no evidence of estimation concerns that would raise concerns for the validity of the results. As a result, we can look at the results from the model overall.

```{r RaschResults}
mcmc_plot(Rasch_inter, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_Item", regex = TRUE)
pp_check(Rasch_inter, ndraws = 50, type = "bars")
pp_check(Rasch_inter, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(Rasch_inter, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Even with no predictors and fewer parameters in the Rasch model, the model does very well in predicting responses. The density plot of the item easiness estimates demonstrates that most items are fairly easy. The following general model summary integrates basic model validity statistics and posterior summaries for additional parameters.

```{r RaschSummary}
summary(Rasch_inter)
```

To help clarify the meaning of some major elements of the above output, consider the following guide:
1. "Estimate" refers to the average posterior value for the parameter,
2. "Est. Error" is the standard deviation of the posterior distribution,
3. "l-95% CI" is the lower bound of the 95% credible interval,
4. "u-95% CI" is the upper bound of the 95% credible interval,
5. "Rhat" is the $\hat{R}$ value for that parameter (rounded to two decimal places),
6. "Bulk_ESS" is the effective sample size based on rank normalized draws and estimates the sampling efficiency of the mean of the posterior, and
7. "Tail_ESS" is the minimum of the effect sample sizes in the 5% and 95% quantiles.

## 2PL Fixed Items

```{r 2PLInterPlots}
mcmc_plot(TwoPL_inter, type = "nuts_divergence")
mcmc_plot(TwoPL_inter, type = "nuts_treedepth")
mcmc_plot(TwoPL_inter, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_inter, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_inter, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_inter, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_inter, type = "neff_hist", binwidth = 0.1)
```

The fixed item 2PL model demonstrated no evidence of estimation concerns that would raise concerns for the validity of the results. As a result, we can look at the results from the model overall.

```{r 2PLInterResults}
mcmc_plot(TwoPL_inter, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_inter, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_inter, ndraws = 50, type = "bars")
pp_check(TwoPL_inter, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_inter, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Much like with the Rasch model, the 2PL fixed item model does very well in predicting responses. There are two parameters (one for difficulty/beta and one for discrimination/alpha) being estimated since the 2PL model is a non-linear model. Note that the model's name for the discrimination parameter is "logalpha." This name reflects the fact that the alpha parameter was log-transformed to ensure that it was constrained in estimation to positive values. The following general model summary integrates basic model validity statistics and posterior summaries for additional parameters. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLInterSummary}
summary(TwoPL_inter)
```

## Multidimensional 2PL Model

```{r 2PLMultiPlots}
mcmc_plot(TwoPL_multi, type = "nuts_divergence")
mcmc_plot(TwoPL_multi, type = "nuts_treedepth")
mcmc_plot(TwoPL_multi, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_multi, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_multi, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_multi, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_multi, type = "neff_hist", binwidth = 0.1)
```

The 2PL model with a unique factor per trial shows no significant modeling concerns. Since there are no validity concerns, we can look at the results from the model overall.

```{r 2PLMultiResults}
mcmc_plot(TwoPL_multi, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_multi, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_multi, ndraws = 50, type = "bars")
pp_check(TwoPL_multi, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_multi, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Like the other 2PL models, this model estimates responses with high accuracy. The following general model summary integrates basic model validity statistics and posterior summaries for additional parameters. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLMultiSummary}
summary(TwoPL_multi)
```

## Latent Change Model

```{r 2PLChangePlots}
mcmc_plot(TwoPL_chnge, type = "nuts_divergence")
mcmc_plot(TwoPL_chnge, type = "nuts_treedepth")
mcmc_plot(TwoPL_chnge, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_chnge, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_chnge, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_chnge, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_chnge, type = "neff_hist", binwidth = 0.1)
```

The 2PL model with factors for change over each trial shows no significant modeling concerns either. Since there are no validity concerns, we can look at the results from the model overall.

```{r 2PLChangeResults}
mcmc_plot(TwoPL_chnge, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_chnge, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_chnge, ndraws = 50, type = "bars")
pp_check(TwoPL_chnge, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_chnge, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Consistent with the other 2PL models so far, the model appears to predict the data generation process closely. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLChangeSummary}
summary(TwoPL_chnge)
```

## Latent Growth (Learning) Model

```{r 2PLLearnPlots}
mcmc_plot(TwoPL_learn, type = "nuts_divergence")
mcmc_plot(TwoPL_learn, type = "nuts_treedepth")
mcmc_plot(TwoPL_learn, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_learn, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_learn, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_learn, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_learn, type = "neff_hist", binwidth = 0.1)
```

The 2PL model with a growth/learning factor also yields no estimation errors. Since there are no validity concerns for the model, we can look at the results from the model.

```{r 2PLLearnResults}
mcmc_plot(TwoPL_learn, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_learn, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_learn, ndraws = 50, type = "bars")
pp_check(TwoPL_learn, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_learn, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Continuing the trends of the other models, these posterior checks closely align with the observed data. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLLearnSummary}
summary(TwoPL_learn)
```

## Local Dependency Model

```{r 2PLDependPlots}
mcmc_plot(TwoPL_depmd, type = "nuts_divergence")
mcmc_plot(TwoPL_depmd, type = "nuts_treedepth")
mcmc_plot(TwoPL_depmd, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_depmd, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_depmd, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_depmd, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_depmd, type = "neff_hist", binwidth = 0.1)
```

The local dependency effect on the 2PL model fit without any modeling concerns. Given the lack of validity issues, the results from the model overall are shown next.

```{r 2PLDependResults}
mcmc_plot(TwoPL_depmd, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_depmd, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_depmd, ndraws = 50, type = "bars")
pp_check(TwoPL_depmd, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_depmd, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

As with the other models, the predictive checks closely align to the data. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLDependSummary}
summary(TwoPL_depmd)
```

## Local Dependency by Item Model

```{r 2PLUniquePlots}
mcmc_plot(TwoPL_depun, type = "nuts_divergence")
mcmc_plot(TwoPL_depun, type = "nuts_treedepth")
mcmc_plot(TwoPL_depun, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_depun, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_depun, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_depun, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_depun, type = "neff_hist", binwidth = 0.1)
```

As with the model with a uniform local dependency effect, the model with a unique dependency effect per item demonstrated no estimation errors, so next is shown the results from the model overall.

```{r 2PLUniqueResults}
mcmc_plot(TwoPL_depun, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_depun, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_depun, ndraws = 50, type = "bars")
pp_check(TwoPL_depun, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_depun, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

These posterior predictive checks also strongly align with the observed data. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLUniqueSummary}
summary(TwoPL_depun)
```

## Local Dependency by Trial Model

```{r 2PLTrialPlots}
mcmc_plot(TwoPL_deptr, type = "nuts_divergence")
mcmc_plot(TwoPL_deptr, type = "nuts_treedepth")
mcmc_plot(TwoPL_deptr, type = "trace", variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_deptr, type = "trace", variable = "b_logalpha_Item", regex = TRUE)
mcmc_plot(TwoPL_deptr, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_deptr, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_deptr, type = "neff_hist", binwidth = 0.1)
```

Decomposing the dependency matrix into unique trial effects, rather than unique item effects, also does not cause any modeling validity concerns. As with the other models, the overall results are shown next.

```{r 2PLTrialResults}
mcmc_plot(TwoPL_deptr, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta_Item", regex = TRUE)
mcmc_plot(TwoPL_deptr, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha_Item", regex = TRUE)
pp_check(TwoPL_deptr, ndraws = 50, type = "bars")
pp_check(TwoPL_deptr, ndraws = 50, type = "bars_grouped", group = "Item")
pp_check(TwoPL_deptr, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Posterior predictive checks are consistent with the other models in that the model predictions and observed data closely align. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLTrialSummary}
summary(TwoPL_deptr)
```

## Item Covariates

```{r 2PLItemCovPlots}
mcmc_plot(TwoPL_itmex, type = "nuts_divergence")
mcmc_plot(TwoPL_itmex, type = "nuts_treedepth")
mcmc_plot(TwoPL_itmex, type = "trace", variable = "b_beta", regex = TRUE)
mcmc_plot(TwoPL_itmex, type = "trace", variable = "b_logalpha", regex = TRUE)
mcmc_plot(TwoPL_itmex, type = "trace", variable = "sd_", regex = TRUE)
mcmc_plot(TwoPL_itmex, type = "rhat_hist", binwidth = 0.0001)
mcmc_plot(TwoPL_itmex, type = "neff_hist", binwidth = 0.1)
```

The 2PL model with all item covariates demonstrated no evidence of estimation concerns that would raise concerns for the validity of the results. Since there are no validity concerns, we can look at the results from the model overall.

```{r 2PLItemCovResults}
mcmc_plot(TwoPL_itmex, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_beta", regex = TRUE)
mcmc_plot(TwoPL_itmex, type = "areas_ridges", prob = 0.80, prob_outer = 0.95, variable = "b_logalpha", regex = TRUE)
pp_check(TwoPL_itmex, ndraws = 50, type = "bars")
pp_check(TwoPL_itmex, ndraws = 50, type = "bars_grouped", group = "ID", newdata = subset(df_long, df_long$ID %in% as.factor(sample.int(n = 1219, size = 20, replace = FALSE))))
```

Like the other 2PL models, this model estimates responses with high accuracy. The following general model summary integrates basic model validity statistics and posterior summaries for additional parameters. See the guide at the end of the Rasch intercept tab for details regarding the meaning of each value.

```{r 2PLItemCovSummary}
summary(TwoPL_itmex)
```

# Final Model Details

This final section provides further details on the final item covariate model identified in this study. To begin with, some additional details regarding the model's performance and fit are provided for readers. The manuscript provides summaries of coefficients as well as item and person fit statistics. In a previous section, this supplemental manuscript provided visual summaries of the coefficients as well. To further enrich the visual understanding of the final data, the following plots describe item and person fit further:

```{r FitStats, echo = FALSE}
item_fit <- readRDS("ItemFit.rds")
person_fit <- readRDS("PersonFit.rds")

windowsFonts(Times=windowsFont("Times New Roman"))

theme_hist <- function(...) {
  bayesplot::theme_default() +
    theme(
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank(),
      ...
    )
}

item_fit %>% 
  ggplot(aes(crit_diff)) +
  geom_histogram() +
  facet_wrap("Item", scales = "free") +
  theme_hist()  +
  theme(text=element_text(family="Times", face="bold", size=12))
person_diff <- person_fit %>%
  group_by(ID) %>%
  summarise(bp = mean(crit_diff > 0))
person_max <- which.max(person_diff$bp)
person_fit %>% 
  filter(ID == person_max) %>%
  ggplot(aes(crit_diff)) +
  geom_histogram() +
  theme_hist() +
  theme(text=element_text(family="Times", face="bold", size=12))
```

These two plots show the empirical distribution of the differences in log-likelihood of the predicted and observed data under the models. The first plot is grouped by items and reflects the item fit. In an ideal model, the distribution is normally distributed with a mean of 0. To address the impracticality of plotting all of these plots for each of 1219 participants, just the most extreme case (individual with worst fit) is shown. While the aim of this study was to examine predictors of item traits, it is still possible to visualize person traits (i.e., memory). The following plot shows the ordered distribution of latent trait estimates and their 95% credible intervals for each participant in the study:

```{r PersonPlots, echo = FALSE}
person_pars <- readRDS("PersonEstimates.rds")

person_pars %>%
  arrange(Estimate) %>%
  mutate(id2 = seq_len(n())) %>%
  ggplot(aes(id2, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Person Number (sorted after Estimate)") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  theme(text=element_text(family="Times", face="bold", size=12))
```

The plots demonstrate that, on the whole, the estimates of a person's latent trait is fairly wide. This is not entirely surprising given that the only estimate being given to the latent trait is a random intercept for each person. The explanatory item response theory model will be expanded in a future study to include factors that predict latent trait, which may reduce uncertainty in the latent trait measurement. The largest limiter to higher accuracy in the latent trait estimate is the fact that memory is being measured with just 10 items repeated only 3 times, so person predictors are likely not going to be a significant source of error reduction. It is also potentially useful to visualize all the conditional effects from the final model. These plots are shown next:

```{r FinalConditionalEffects, echo = FALSE}
plot(conditional_effects(TwoPL_itmex), ask = FALSE)
```

There are also some general test descriptives that are usually helpful to examine. These include test reliability, the expected score function, and test information. The following plots are these visualizations in their respective order:

```{r ExtraPlots, echo = FALSE}
theta_est <- as.data.frame(ranef(TwoPL_deptr, summary = TRUE)$ID)[, 1:2]
var_theta <- as.numeric(VarCorr(TwoPL_deptr)[[1]][[1]][1])
reliability_est <- var_theta/(var_theta + theta_est[, 2]^2)
rel_dat <- data.frame(Theta = theta_est[, 1], Rxx = reliability_est)
rm(theta_est, var_theta, reliability_est)
ggplot(data = rel_dat, aes(x = Theta, y = Rxx)) +
  geom_smooth(color = "black", size = 1.10) +
  ylab("Reliability Estimate") +
  xlab("Person Ability Estimate") +
  theme_bw() +
  theme(text=element_text(family="Times", face="bold", size=12))

fixed <- as.data.frame(fixef(TwoPL_deptr, summary = FALSE))

temp <- fixed %>%
  select(contains("Item")) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(starts_with(c("beta_", "logalpha_"))) %>%
  mutate(Item   = str_remove(name, "^beta_Item|^logalpha_Item"),
         Parameter = ifelse(str_detect(name, "beta"), "Difficulty", "Discrimination")) %>%
  select(-name) %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  expand(nesting(iter, Item, Difficulty, Discrimination),
         Theta = seq(-6, 6, length.out = 100)) %>%
  mutate(p = inv_logit_scaled(Difficulty + exp(Discrimination) * Theta)) %>%
  group_by(Theta, Item) %>%
  summarise(ci = list(as.data.frame(posterior_summary(p)) %>% 
                        rename(p = Estimate, ymin = Q2.5, ymax = Q97.5))) %>% 
  unnest(cols = c(ci)) %>%
  select(-Est.Error) %>%
  mutate(Trial = rep(1:3, times = 10),
         Item  = str_remove(Item, "\\d$"))

fixed1 <- fixed
fixed1[, 13:22] <- fixed[, 13:22]+fixed[, 1]
fixed1[, 23:32] <- fixed[, 23:32]+fixed[, 2]
fixed1[, 45:54] <- fixed[, 45:54]+fixed[, 33]
fixed1[, 55:64] <- fixed[, 55:64]+fixed[, 34]

temp1 <- fixed1 %>%
  select(contains("Item")) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(starts_with(c("beta_", "logalpha_"))) %>%
  mutate(Item   = str_remove(name, "^beta_Item|^logalpha_Item"),
         Parameter = ifelse(str_detect(name, "beta"), "Difficulty", "Discrimination")) %>%
  select(-name) %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  expand(nesting(iter, Item, Difficulty, Discrimination),
         Theta = seq(-6, 6, length.out = 100)) %>%
  mutate(p = inv_logit_scaled(Difficulty + exp(Discrimination) * Theta)) %>%
  group_by(Theta, Item) %>%
  summarise(ci = list(as.data.frame(posterior_summary(p)) %>% 
                        rename(p = Estimate, ymin = Q2.5, ymax = Q97.5))) %>% 
  unnest(cols = c(ci)) %>%
  select(-Est.Error) %>%
  mutate(Trial = rep(1:3, times = 10),
         Item  = str_remove(Item, "\\d$"))

ICCpars <- rbind(temp, temp1)
ICCpars$Dependency <- rep(c("No Prior Recall", "Previous Recall"), each = 3000)
ICCpars <- ICCpars[, c(6, 2, 1, 3, 4:5, 7)]

TESpars <- ICCpars %>%
  group_by(Theta, Trial, Dependency) %>%
  summarise(Expected = sum(p))

TESpars[TESpars$Trial == 1 & TESpars$Dependency == "No Prior Recall", ] %>%
  ggplot(aes(x = Theta, y = Expected)) +
  geom_line(size = 1.05) +
  labs(title = "Expected Score Function for Trial 1",
       x = expression(theta~('ability on the logit scale')),
       y = "Expected Raw Score") +
  ylim(0, 10) +
  theme_classic()

TESpars[TESpars$Trial == 2, ] %>%
  ggplot(aes(x = Theta, y = Expected, linetype = Dependency)) +
  geom_line(size = 1.05) +
  labs(title = "Expected Score Function for Trial 2",
       x = expression(theta~('ability on the logit scale')),
       y = "Expected Raw Score") +
  ylim(0, 10) +
  theme_classic()

TESpars[TESpars$Trial == 3, ] %>%
  ggplot(aes(x = Theta, y = Expected, linetype = Dependency)) +
  geom_line(size = 1.05) +
  labs(title = "Expected Score Function for Trial 3",
       x = expression(theta~('ability on the logit scale')),
       y = "Expected Raw Score") +
  ylim(0, 10) +
  theme_classic()

temp <- fixed %>%
  select(contains("Item")) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(starts_with(c("beta_", "logalpha_"))) %>%
  mutate(Item   = str_remove(name, "^beta_Item|^logalpha_Item"),
         Parameter = ifelse(str_detect(name, "beta"), "Difficulty", "Discrimination")) %>%
  select(-name) %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  expand(nesting(iter, Item, Difficulty, Discrimination),
         Theta = seq(-6, 6, length.out = 100)) %>%
  mutate(p = inv_logit_scaled(Difficulty + exp(Discrimination) * Theta)) %>%
  mutate(i = p * (1 - p)) %>% 
  group_by(Theta, Item) %>% 
  summarise(ci = list(as.data.frame(posterior_summary(i)) %>% 
                        rename(i = Estimate, ymin = Q2.5, ymax = Q97.5))) %>% 
  unnest(cols = c(ci)) %>%
  select(-Est.Error) %>%
  mutate(Trial = rep(1:3, times = 10),
         Item  = str_remove(Item, "\\d$"))

temp1 <- fixed1 %>%
  select(contains("Item")) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(starts_with(c("beta_", "logalpha_"))) %>%
  mutate(Item   = str_remove(name, "^beta_Item|^logalpha_Item"),
         Parameter = ifelse(str_detect(name, "beta"), "Difficulty", "Discrimination")) %>%
  select(-name) %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  expand(nesting(iter, Item, Difficulty, Discrimination),
         Theta = seq(-6, 6, length.out = 100)) %>%
  mutate(p = inv_logit_scaled(Difficulty + exp(Discrimination) * Theta)) %>%
  mutate(i = p * (1 - p)) %>% 
  group_by(Theta, Item) %>% 
  summarise(ci = list(as.data.frame(posterior_summary(i)) %>% 
                        rename(i = Estimate, ymin = Q2.5, ymax = Q97.5))) %>% 
  unnest(cols = c(ci)) %>%
  select(-Est.Error) %>%
  mutate(Trial = rep(1:3, times = 10),
         Item  = str_remove(Item, "\\d$"))

IICpars <- rbind(temp, temp1)
IICpars$Dependency <- rep(c("No Prior Recall", "Previous Recall"), each = 3000)
IICpars <- IICpars[, c(6, 2, 1, 3, 4:5, 7)]

IICpars[IICpars$Trial == 1 & IICpars$Dependency == "No Prior Recall", ] %>%
  mutate(Item = factor(Item, levels = c("Butter", "Arm", "Shore", "Letter", "Queen", "Cabin", "Pole", "Ticket", "Grass", "Engine"))) %>%
  ggplot(aes(x = Theta, y = i)) +
  geom_line(size = 1.05) +
  geom_ribbon(aes(ymin = ymin, ymax = ymax), alpha = 0.25) +
  facet_wrap(~ Item, ncol = 5) +
  labs(title = "IICs for the 2PL",
       subtitle = "Each curve is based on the posterior median.", 
       x = expression(theta~('ability on the logit scale')),
       y = "Information") +
  theme_classic()

IICpars[IICpars$Trial == 2, ] %>%
  mutate(Item = factor(Item, levels = c("Ticket", "Cabin", "Butter", "Shore", "Engine", "Arm", "Queen", "Letter", "Pole", "Grass"))) %>%
  ggplot(aes(x = Theta, y = i, linetype = Dependency)) +
  geom_line(size = 1.05) +
  geom_ribbon(aes(ymin = ymin, ymax = ymax), alpha = 0.25) +
  facet_wrap(~ Item, ncol = 5) +
  labs(title = "IICs for the 2PL",
       subtitle = "Each curve is based on the posterior median.", 
       x = expression(theta~('ability on the logit scale')),
       y = "Information") +
  theme_classic()
  
IICpars[IICpars$Trial == 3, ] %>%
  mutate(Item = factor(Item, levels = c("Queen", "Grass", "Arm", "Cabin", "Pole", "Shore", "Butter", "Engine", "Ticket", "Letter"))) %>%
  ggplot(aes(x = Theta, y = i, linetype = Dependency)) +
  geom_line(size = 1.05) +
  geom_ribbon(aes(ymin = ymin, ymax = ymax), alpha = 0.25) +
  facet_wrap(~ Item, ncol = 5) +
  labs(title = "IICs for the 2PL",
       subtitle = "Each curve is based on the posterior median.", 
       x = expression(theta~('ability on the logit scale')),
       y = "Information") +
  theme_classic()

temp <- fixed %>%
  select(contains("Item")) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(starts_with(c("beta_", "logalpha_"))) %>%
  mutate(Item   = str_remove(name, "^beta_Item|^logalpha_Item"),
         Parameter = ifelse(str_detect(name, "beta"), "Difficulty", "Discrimination")) %>%
  select(-name) %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  expand(nesting(iter, Item, Difficulty, Discrimination),
         Theta = seq(-6, 6, length.out = 100)) %>%
  mutate(p = inv_logit_scaled(Difficulty + exp(Discrimination) * Theta)) %>%
  mutate(i = p * (1 - p)) %>% 
  group_by(Theta, iter) %>% 
  summarise(sum_i = sum(i)) %>% 
  group_by(Theta) %>% 
  summarise(ci = list(as.data.frame(posterior_summary(sum_i)) %>% 
                        rename(i = Estimate, ymin = Q2.5, ymax = Q97.5))) %>% 
  unnest(cols = c(ci)) %>%
  select(-Est.Error)

temp1 <- fixed1 %>%
  select(contains("Item")) %>%
  mutate(iter = 1:n()) %>%
  pivot_longer(starts_with(c("beta_", "logalpha_"))) %>%
  mutate(Item   = str_remove(name, "^beta_Item|^logalpha_Item"),
         Parameter = ifelse(str_detect(name, "beta"), "Difficulty", "Discrimination")) %>%
  select(-name) %>%
  pivot_wider(names_from = Parameter, values_from = value) %>%
  expand(nesting(iter, Item, Difficulty, Discrimination),
         Theta = seq(-6, 6, length.out = 100)) %>%
  mutate(p = inv_logit_scaled(Difficulty + exp(Discrimination) * Theta)) %>%
  mutate(i = p * (1 - p)) %>% 
  group_by(Theta, iter) %>% 
  summarise(sum_i = sum(i)) %>% 
  group_by(Theta) %>% 
  summarise(ci = list(as.data.frame(posterior_summary(sum_i)) %>% 
                        rename(i = Estimate, ymin = Q2.5, ymax = Q97.5))) %>% 
  unnest(cols = c(ci)) %>%
  select(-Est.Error)

IICtest <- rbind(temp, temp1)
IICtest$Dependency <- rep(c("No Prior Recall", "Previous Recall"), each = 100)

IICtest %>%
  ggplot(aes(x = Theta, y = i, linetype = Dependency)) +
  geom_line(size = 1.05) +
  geom_ribbon(aes(ymin = ymin, ymax = ymax), alpha = 0.25) +
  labs(title = "IIC for the 2PL",
       subtitle = "Curve is based on the posterior median.", 
       x = expression(theta~('ability on the logit scale')),
       y = "Information") +
  theme_classic()
```

Within IRT, information and reliability are very closely related. It is clear that these plots both have generally the same shape with peaks in their functions around slightly below average scores. As is clear from both of these scales, the CERAD word list is not ideal for measuring individuals with either very low or very high memory abilities. In dementia/mild cognitive impairment research, this particular trait may be favorable as the goal is often to characterize with greater precision mildly impaired memory with relatively little need for carefully measuring extremes. The expected score function is a useful visual analogue for the relationship between raw score and latent trait estimate. The relationship is clearly monotonic and overall very gradual, which means that the raw scores do span a relatively wide range of latent trait values.
